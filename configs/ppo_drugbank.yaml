# common data setting
max_length: 256
dataset: 'drugbank'
pretrained_model_path: './pretrained_model_path/roberta-base' #
ddi_dict_path: './data/ddi/DDI_dict_action_roberta.json'
annotation: 'ppo_zero_shot_drugbank_dev'
initialize_checkpoint: './checkpoints/06-19/drugbank_env1/checkpoint_epoch6.pt'
policy_resume: True
policy_checkpoint: './checkpoints/06-18/ppo_zero_shot_drugbank_rl_0/checkpoint_epoch10500.pt'
use_zero_shot : True

# ppo setting
# num_process: 1
use_gae: True
gamma: 0.99
gae_lambda: 0.95
ppo_epoch: 1
num_mini_batch: 4
clip_param: 0.1
value_loss_coef: 0.5
entropy_coef: 0.01
max_grad_norm: 0.5
use_clipped_value_loss: True

# train eval setting
multi_gpu: True
eval: False
# random_act: True
eval_before_train: True
eval_vanilla_before_train: False
eval_step: 1500
use_stop: False

use_gpu: True
deepspeed: False
gpuid: '0,1,2,3,4,5,6,7'
gpu_num: 8
test: True
filemode: 'w'
patience : 5
num_train_epochs: 100
eval_actions_batch_size: 2048 # 4096
train_actions_batch_size: 128
eval_vanilla_batch_size: 512
per_gpu_train_batch_size: 1 # fake
gradient_accumulation_steps: 1 


log_step: 1
lr_scheduler_type: 'constant' # constant cosine
fp16: False
lr: 1.0e-5
weight_decay: 1.0e-6  # 1.0e-6 #1e-2
epsilon: 1.0e-8
seed: 2023